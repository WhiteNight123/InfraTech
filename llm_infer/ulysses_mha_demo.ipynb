{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMuWWtdApy5KshL91Ej/gWt"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Ulysses 序列并行示例\n",
        "\n",
        "Author: kaiyuan\n",
        "\n",
        "Email: kyxie@zju.edu.cn"
      ],
      "metadata": {
        "id": "E4JSMpRh3FFG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**说明**\n",
        "\n",
        "仅关注前向运算。先定义一个标准Attention运算作为参照，再定义一个具备ulysses原理的运算过程。关键点：\n",
        "* 用for循环来模拟多个GPU运算；\n",
        "* 定义两个函数：模拟attention计算的前后alltoall过程；\n",
        "* 为了方便结果比较，序列并行体现在Attention运算内部，最后计算结果cat到一起。"
      ],
      "metadata": {
        "id": "j9Z-cmM83hUj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cHVrtlo1zLi2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class StandardAttention(nn.Module):\n",
        "    \"\"\"方式1：标准多头注意力\"\"\"\n",
        "    def __init__(self, hidden_dim=8, num_heads=2):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = hidden_dim // num_heads\n",
        "\n",
        "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.k_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.v_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.out_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.shape[0]\n",
        "\n",
        "        print(\"=\"*60)\n",
        "        print(\"方式1：标准多头注意力\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"1. 输入 shape: {x.shape}\")\n",
        "\n",
        "        # Q/K/V投影\n",
        "        Q = self.q_proj(x)\n",
        "        K = self.k_proj(x)\n",
        "        V = self.v_proj(x)\n",
        "        print(f\"2. Q/K/V投影 shape: {Q.shape}\")\n",
        "\n",
        "        # 重塑为多头\n",
        "        Q = Q.view(seq_len, self.num_heads, self.head_dim)\n",
        "        K = K.view(seq_len, self.num_heads, self.head_dim)\n",
        "        V = V.view(seq_len, self.num_heads, self.head_dim)\n",
        "        print(f\"3. 重塑多头 shape: {Q.shape}\")\n",
        "\n",
        "        # 转置用于注意力计算\n",
        "        Q = Q.transpose(0, 1)  # [num_heads, seq_len, head_dim]\n",
        "        K = K.transpose(0, 1)\n",
        "        V = V.transpose(0, 1)\n",
        "        print(f\"4. 转置后 shape: {Q.shape}\")\n",
        "\n",
        "        # 注意力计算（包含softmax）\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "        attn_scores = torch.softmax(attn_scores, dim=-1)  # 加上softmax\n",
        "        attn_output = torch.matmul(attn_scores, V)\n",
        "        print(f\"5. 注意力计算后 shape: {attn_output.shape}\")\n",
        "\n",
        "        # 转置回来\n",
        "        attn_output = attn_output.transpose(0, 1)  # [seq_len, num_heads, head_dim]\n",
        "        print(f\"6. 转置回来 shape: {attn_output.shape}\")\n",
        "\n",
        "        # 重塑回原始形状\n",
        "        attn_output = attn_output.reshape(seq_len, self.hidden_dim)\n",
        "        print(f\"7. 重塑回原始 shape: {attn_output.shape}\")\n",
        "\n",
        "        # 输出投影\n",
        "        output = self.out_proj(attn_output)\n",
        "        print(f\"8. 最终输出 shape: {output.shape}\")\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class UlyssesParallelAttention(nn.Module):\n",
        "    \"\"\"方式2：Ulysses序列并行\"\"\"\n",
        "    def __init__(self, hidden_dim=8, num_heads=2, num_gpus=2):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.num_gpus = num_gpus\n",
        "        self.head_dim = hidden_dim // num_heads\n",
        "        self.local_num_heads = num_heads // num_gpus\n",
        "\n",
        "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.k_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.v_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.out_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "    def all_to_all_head_to_sequence(self, data_list):\n",
        "        \"\"\"模拟All-to-All通信：从头维度交换到序列维度\"\"\"\n",
        "        # data_list: 每个GPU的数据 [local_seq_len, num_heads, head_dim]\n",
        "        # 返回: 每个GPU的数据 [seq_len, local_num_heads, head_dim]\n",
        "\n",
        "        num_gpus = len(data_list)\n",
        "        local_seq_len = data_list[0].shape[0]\n",
        "        seq_len = local_seq_len * num_gpus\n",
        "\n",
        "        results = []\n",
        "        for gpu_i in range(num_gpus):\n",
        "            # 收集来自所有GPU的第gpu_i个头的部分\n",
        "            parts = []\n",
        "            for gpu_j in range(num_gpus):\n",
        "                # 从GPU_j获取对应头的部分\n",
        "                part = data_list[gpu_j][:, gpu_i*self.local_num_heads:(gpu_i+1)*self.local_num_heads, :]\n",
        "                parts.append(part)\n",
        "\n",
        "            # 在序列维度拼接\n",
        "            result = torch.cat(parts, dim=0)  # [seq_len, local_num_heads, head_dim]\n",
        "            results.append(result)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def all_to_all_sequence_to_head(self, data_list):\n",
        "        \"\"\"模拟All-to-All通信：从序列维度交换到头维度\"\"\"\n",
        "        # data_list: 每个GPU的数据 [seq_len, local_num_heads, head_dim]\n",
        "        # 返回: 每个GPU的数据 [local_seq_len, num_heads, head_dim]\n",
        "\n",
        "        num_gpus = len(data_list)\n",
        "        seq_len = data_list[0].shape[0]\n",
        "        local_seq_len = seq_len // num_gpus\n",
        "\n",
        "        results = []\n",
        "        for gpu_i in range(num_gpus):\n",
        "            # 收集来自所有GPU的第gpu_i个序列部分\n",
        "            parts = []\n",
        "            for gpu_j in range(num_gpus):\n",
        "                # 从GPU_j获取对应序列的部分\n",
        "                part = data_list[gpu_j][gpu_i*local_seq_len:(gpu_i+1)*local_seq_len, :, :]\n",
        "                parts.append(part)\n",
        "\n",
        "            # 在头维度拼接\n",
        "            result = torch.cat(parts, dim=1)  # [local_seq_len, num_heads, head_dim]\n",
        "            results.append(result)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.shape[0]\n",
        "        local_seq_len = seq_len // self.num_gpus\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"方式2：Ulysses序列并行\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"参数: seq_len={seq_len}, 每个GPU local_seq_len={local_seq_len}\")\n",
        "        print(f\"      num_heads={self.num_heads}, 每个GPU local_num_heads={self.local_num_heads}\")\n",
        "        print()\n",
        "        print(f\"1. 完整输入 shape: {x.shape}\")\n",
        "\n",
        "        # 1. 分割序列到不同GPU\n",
        "        x_split = torch.chunk(x, self.num_gpus, dim=0)\n",
        "        print(f\"2. 序列分割到{self.num_gpus}个GPU:\")\n",
        "        for i in range(self.num_gpus):\n",
        "            print(f\"   GPU{i} 输入 shape: {x_split[i].shape}\")\n",
        "\n",
        "        # 2. 每个GPU本地计算Q/K/V\n",
        "        print(f\"\\n3. 每个GPU计算本地Q/K/V:\")\n",
        "        q_list, k_list, v_list = [], [], []\n",
        "\n",
        "        for i in range(self.num_gpus):\n",
        "            x_local = x_split[i]\n",
        "\n",
        "            # 本地投影\n",
        "            Q_local = self.q_proj(x_local)\n",
        "            K_local = self.k_proj(x_local)\n",
        "            V_local = self.v_proj(x_local)\n",
        "            print(f\"   GPU{i}: Q/K/V投影 shape: {Q_local.shape}\")\n",
        "\n",
        "            # 重塑为多头\n",
        "            Q_local = Q_local.view(local_seq_len, self.num_heads, self.head_dim)\n",
        "            K_local = K_local.view(local_seq_len, self.num_heads, self.head_dim)\n",
        "            V_local = V_local.view(local_seq_len, self.num_heads, self.head_dim)\n",
        "            print(f\"   GPU{i}: 重塑多头 shape: {Q_local.shape}\")\n",
        "\n",
        "            q_list.append(Q_local)\n",
        "            k_list.append(K_local)\n",
        "            v_list.append(V_local)\n",
        "\n",
        "        # 3. 第一次All-to-All：从头维度交换到序列维度\n",
        "        print(f\"\\n4. 第一次All-to-All通信:\")\n",
        "        print(f\"   前: 每个GPU shape {q_list[0].shape}\")\n",
        "\n",
        "        Q_exchanged = self.all_to_all_head_to_sequence(q_list)\n",
        "        K_exchanged = self.all_to_all_head_to_sequence(k_list)\n",
        "        V_exchanged = self.all_to_all_head_to_sequence(v_list)\n",
        "\n",
        "        print(f\"   后: 每个GPU shape {Q_exchanged[0].shape}\")\n",
        "\n",
        "        # 4. 每个GPU计算注意力（包含softmax）\n",
        "        print(f\"\\n5. 每个GPU计算注意力:\")\n",
        "        attn_outputs = []\n",
        "\n",
        "        for i in range(self.num_gpus):\n",
        "            Q_ex = Q_exchanged[i]\n",
        "            K_ex = K_exchanged[i]\n",
        "            V_ex = V_exchanged[i]\n",
        "\n",
        "            # 转置用于注意力计算\n",
        "            Q_ex = Q_ex.transpose(0, 1)  # [local_num_heads, seq_len, head_dim]\n",
        "            K_ex = K_ex.transpose(0, 1)\n",
        "            V_ex = V_ex.transpose(0, 1)\n",
        "            print(f\"   GPU{i}: 转置后 shape: {Q_ex.shape}\")\n",
        "\n",
        "            # 注意力计算\n",
        "            attn_scores = torch.matmul(Q_ex, K_ex.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "            attn_scores = torch.softmax(attn_scores, dim=-1)\n",
        "            attn_output = torch.matmul(attn_scores, V_ex)\n",
        "            print(f\"   GPU{i}: 注意力计算后 shape: {attn_output.shape}\")\n",
        "\n",
        "            # 转置回来\n",
        "            attn_output = attn_output.transpose(0, 1)  # [seq_len, local_num_heads, head_dim]\n",
        "            print(f\"   GPU{i}: 转置回来 shape: {attn_output.shape}\")\n",
        "\n",
        "            attn_outputs.append(attn_output)\n",
        "\n",
        "        # 5. 第二次All-to-All：从序列维度交换回头维度\n",
        "        print(f\"\\n6. 第二次All-to-All通信:\")\n",
        "        print(f\"   前: 每个GPU shape {attn_outputs[0].shape}\")\n",
        "\n",
        "        attn_exchanged = self.all_to_all_sequence_to_head(attn_outputs)\n",
        "\n",
        "        print(f\"   后: 每个GPU shape {attn_exchanged[0].shape}\")\n",
        "\n",
        "        # 6. 重塑并投影输出\n",
        "        print(f\"\\n7. 每个GPU重塑并投影:\")\n",
        "        final_outputs = []\n",
        "\n",
        "        for i in range(self.num_gpus):\n",
        "            attn_local = attn_exchanged[i]\n",
        "\n",
        "            # 重塑为 [local_seq_len, hidden_dim]\n",
        "            attn_reshaped = attn_local.reshape(local_seq_len, self.hidden_dim)\n",
        "            print(f\"   GPU{i}: 重塑后 shape: {attn_reshaped.shape}\")\n",
        "\n",
        "            # 输出投影\n",
        "            output_local = self.out_proj(attn_reshaped)\n",
        "            print(f\"   GPU{i}: 投影后 shape: {output_local.shape}\")\n",
        "\n",
        "            final_outputs.append(output_local)\n",
        "\n",
        "        # 7. 收集所有GPU输出\n",
        "        full_output = torch.cat(final_outputs, dim=0)\n",
        "        print(f\"\\n8. 收集所有GPU输出 shape: {full_output.shape}\")\n",
        "\n",
        "        return full_output\n"
      ],
      "metadata": {
        "id": "4058olWG2fk7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_with_same_weights():\n",
        "    \"\"\"使用相同权重对比两种方式\"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"Ulysses序列并行结果对比\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # 设置随机种子\n",
        "    torch.manual_seed(3)\n",
        "\n",
        "    # 参数\n",
        "    seq_len = 2\n",
        "    hidden_dim = 6\n",
        "    num_heads = 2\n",
        "    num_gpus = 2\n",
        "\n",
        "    # 创建相同权重的模型\n",
        "    standard_model = StandardAttention(hidden_dim, num_heads)\n",
        "    parallel_model = UlyssesParallelAttention(hidden_dim, num_heads, num_gpus)\n",
        "\n",
        "    # 复制权重，确保两种方式使用相同的权重\n",
        "    with torch.no_grad():\n",
        "        parallel_model.q_proj.weight.copy_(standard_model.q_proj.weight)\n",
        "        parallel_model.q_proj.bias.copy_(standard_model.q_proj.bias)\n",
        "        parallel_model.k_proj.weight.copy_(standard_model.k_proj.weight)\n",
        "        parallel_model.k_proj.bias.copy_(standard_model.k_proj.bias)\n",
        "        parallel_model.v_proj.weight.copy_(standard_model.v_proj.weight)\n",
        "        parallel_model.v_proj.bias.copy_(standard_model.v_proj.bias)\n",
        "        parallel_model.out_proj.weight.copy_(standard_model.out_proj.weight)\n",
        "        parallel_model.out_proj.bias.copy_(standard_model.out_proj.bias)\n",
        "\n",
        "    # 生成相同的输入数据\n",
        "    x = torch.randn(seq_len, hidden_dim)\n",
        "    print(f\"输入数据 shape: {x.shape}\")\n",
        "    print(f\"输入数据 (前3行):\\n{x[:3]}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "    # 方式1：标准注意力\n",
        "    with torch.no_grad():\n",
        "        output1 = standard_model(x.clone())\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "    # 方式2：Ulysses并行\n",
        "    with torch.no_grad():\n",
        "        output2 = parallel_model(x.clone())\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"结果对比\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # 比较输出\n",
        "    print(f\"方式1输出 shape: {output1.shape}\")\n",
        "    print(f\"方式2输出 shape: {output2.shape}\")\n",
        "\n",
        "    # 计算差异\n",
        "    diff = torch.abs(output1 - output2).max().item()\n",
        "    print(f\"\\n两种方式输出最大差异: {diff:.6f}\")\n",
        "\n",
        "    # 计算相对误差\n",
        "    rel_error = torch.norm(output1 - output2) / torch.norm(output1)\n",
        "    print(f\"相对误差: {rel_error:.6f}\")\n",
        "\n",
        "    # 检查结果是否匹配\n",
        "    if diff < 1e-5:\n",
        "        print(\"✓ 两种方式计算结果一致！\")\n",
        "    else:\n",
        "        print(\"⚠ 两种方式计算结果存在差异\")\n",
        "\n",
        "    # 打印部分输出对比\n",
        "    print(f\"\\n方式1输出 (前3行):\")\n",
        "    print(output1[:3])\n",
        "    print(f\"\\n方式2输出 (前3行):\")\n",
        "    print(output2[:3])\n",
        "\n",
        "    return output1, output2"
      ],
      "metadata": {
        "id": "3Mnyo7hV2j5J"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare_with_same_weights()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ki1r9KT42o9J",
        "outputId": "f4b55917-b16a-4663-f7f9-fde51af90b5d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Ulysses序列并行结果对比\n",
            "============================================================\n",
            "输入数据 shape: torch.Size([2, 6])\n",
            "输入数据 (前3行):\n",
            "tensor([[-0.0991,  1.0835, -0.4738,  0.9102,  0.8597, -0.0252],\n",
            "        [ 0.3625, -1.5558,  0.5427, -0.0135,  1.1126,  1.2475]])\n",
            "\n",
            "============================================================\n",
            "============================================================\n",
            "方式1：标准多头注意力\n",
            "============================================================\n",
            "1. 输入 shape: torch.Size([2, 6])\n",
            "2. Q/K/V投影 shape: torch.Size([2, 6])\n",
            "3. 重塑多头 shape: torch.Size([2, 2, 3])\n",
            "4. 转置后 shape: torch.Size([2, 2, 3])\n",
            "5. 注意力计算后 shape: torch.Size([2, 2, 3])\n",
            "6. 转置回来 shape: torch.Size([2, 2, 3])\n",
            "7. 重塑回原始 shape: torch.Size([2, 6])\n",
            "8. 最终输出 shape: torch.Size([2, 6])\n",
            "\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "方式2：Ulysses序列并行\n",
            "============================================================\n",
            "参数: seq_len=2, 每个GPU local_seq_len=1\n",
            "      num_heads=2, 每个GPU local_num_heads=1\n",
            "\n",
            "1. 完整输入 shape: torch.Size([2, 6])\n",
            "2. 序列分割到2个GPU:\n",
            "   GPU0 输入 shape: torch.Size([1, 6])\n",
            "   GPU1 输入 shape: torch.Size([1, 6])\n",
            "\n",
            "3. 每个GPU计算本地Q/K/V:\n",
            "   GPU0: Q/K/V投影 shape: torch.Size([1, 6])\n",
            "   GPU0: 重塑多头 shape: torch.Size([1, 2, 3])\n",
            "   GPU1: Q/K/V投影 shape: torch.Size([1, 6])\n",
            "   GPU1: 重塑多头 shape: torch.Size([1, 2, 3])\n",
            "\n",
            "4. 第一次All-to-All通信:\n",
            "   前: 每个GPU shape torch.Size([1, 2, 3])\n",
            "   后: 每个GPU shape torch.Size([2, 1, 3])\n",
            "\n",
            "5. 每个GPU计算注意力:\n",
            "   GPU0: 转置后 shape: torch.Size([1, 2, 3])\n",
            "   GPU0: 注意力计算后 shape: torch.Size([1, 2, 3])\n",
            "   GPU0: 转置回来 shape: torch.Size([2, 1, 3])\n",
            "   GPU1: 转置后 shape: torch.Size([1, 2, 3])\n",
            "   GPU1: 注意力计算后 shape: torch.Size([1, 2, 3])\n",
            "   GPU1: 转置回来 shape: torch.Size([2, 1, 3])\n",
            "\n",
            "6. 第二次All-to-All通信:\n",
            "   前: 每个GPU shape torch.Size([2, 1, 3])\n",
            "   后: 每个GPU shape torch.Size([1, 2, 3])\n",
            "\n",
            "7. 每个GPU重塑并投影:\n",
            "   GPU0: 重塑后 shape: torch.Size([1, 6])\n",
            "   GPU0: 投影后 shape: torch.Size([1, 6])\n",
            "   GPU1: 重塑后 shape: torch.Size([1, 6])\n",
            "   GPU1: 投影后 shape: torch.Size([1, 6])\n",
            "\n",
            "8. 收集所有GPU输出 shape: torch.Size([2, 6])\n",
            "\n",
            "============================================================\n",
            "结果对比\n",
            "============================================================\n",
            "方式1输出 shape: torch.Size([2, 6])\n",
            "方式2输出 shape: torch.Size([2, 6])\n",
            "\n",
            "两种方式输出最大差异: 0.000000\n",
            "相对误差: 0.000000\n",
            "✓ 两种方式计算结果一致！\n",
            "\n",
            "方式1输出 (前3行):\n",
            "tensor([[-0.1666,  0.1110, -0.0746, -0.2954, -0.2557,  0.0878],\n",
            "        [-0.1656,  0.1140, -0.0928, -0.3176, -0.2792,  0.1062]])\n",
            "\n",
            "方式2输出 (前3行):\n",
            "tensor([[-0.1666,  0.1110, -0.0746, -0.2954, -0.2557,  0.0878],\n",
            "        [-0.1656,  0.1140, -0.0928, -0.3176, -0.2792,  0.1062]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-0.1666,  0.1110, -0.0746, -0.2954, -0.2557,  0.0878],\n",
              "         [-0.1656,  0.1140, -0.0928, -0.3176, -0.2792,  0.1062]]),\n",
              " tensor([[-0.1666,  0.1110, -0.0746, -0.2954, -0.2557,  0.0878],\n",
              "         [-0.1656,  0.1140, -0.0928, -0.3176, -0.2792,  0.1062]]))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    }
  ]
}